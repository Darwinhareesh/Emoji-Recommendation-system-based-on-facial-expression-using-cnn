{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from flask import Flask, render_template, Response, request, redirect, url_for\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Emotion labels and emoji sets\n",
    "emotion_labels = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', \n",
    "                  4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "emoji_mapping = {\n",
    "    0: ['üò†', 'üò°', 'üò£'],  # Angry\n",
    "    1: ['ü§¢', 'ü§Æ', 'üòñ'],  # Disgust\n",
    "    2: ['üò®', 'üò±', 'üò∞'],  # Fear\n",
    "    3: ['üòä', 'üòÇ', 'ü•≥'],  # Happy\n",
    "    4: ['üò¢', 'üò≠', 'ü•∫'],  # Sad\n",
    "    5: ['üò≤', 'üòØ', 'ü§Ø'],  # Surprise\n",
    "    6: ['üòê', 'üòë', 'üò∂']   # Neutral\n",
    "}\n",
    "\n",
    "# Load your trained model (update with your exact filename)\n",
    "model_path = 'model_weights (1).keras'  # Replace with your model's timestamped filename\n",
    "model = load_model(model_path)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Preprocess image for prediction\n",
    "def preprocess_image(image, target_size=(100, 100)):\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image = img_to_array(image) / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "# Predict emotion and recommend one emoji\n",
    "def predict_emotion(image):\n",
    "    processed_image = preprocess_image(image)\n",
    "    prediction = model.predict(processed_image)\n",
    "    emotion_idx = np.argmax(prediction)\n",
    "    emotion = emotion_labels[emotion_idx]\n",
    "    recommended_emoji = random.choice(emoji_mapping[emotion_idx])\n",
    "    print(f\"Predicted: {emotion} {recommended_emoji}\")  # Debug output\n",
    "    return emotion, recommended_emoji\n",
    "\n",
    "# Webcam feed generator\n",
    "def gen_frames():\n",
    "    camera = cv2.VideoCapture(0)\n",
    "    if not camera.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    if face_cascade.empty():\n",
    "        print(\"Error: Could not load face cascade.\")\n",
    "        return\n",
    "    # Load a font that supports emojis (Windows default or download one)\n",
    "    try:\n",
    "        font_path = \"C:/Windows/Fonts/seguiemj.ttf\"  # Windows emoji font\n",
    "        font = ImageFont.truetype(font_path, 30)\n",
    "    except:\n",
    "        print(\"Error: Could not load font. Download an emoji-supporting font (e.g., Noto Emoji).\")\n",
    "        return\n",
    "    while True:\n",
    "        success, frame = camera.read()\n",
    "        if not success:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        # Convert frame to PIL image for text overlay\n",
    "        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        draw = ImageDraw.Draw(frame_pil)\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            emotion, emoji = predict_emotion(face)\n",
    "            # Draw bounding box with OpenCV\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            # Use PIL to draw text and emoji\n",
    "            display_text = f\"{emotion}: {emoji}\"\n",
    "            draw.text((x, y-40), display_text, font=font, fill=(0, 255, 0, 255))\n",
    "        # Convert back to OpenCV format\n",
    "        frame = cv2.cvtColor(np.array(frame_pil), cv2.COLOR_RGB2BGR)\n",
    "        ret, buffer = cv2.imencode('.jpg', frame)\n",
    "        frame = buffer.tobytes()\n",
    "        yield (b'--frame\\r\\n'\n",
    "               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n",
    "    camera.release()\n",
    "\n",
    "# Routes\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(gen_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "@app.route('/upload', methods=['GET', 'POST'])\n",
    "def upload_file():\n",
    "    if request.method == 'POST':\n",
    "        if 'file' not in request.files:\n",
    "            return redirect(request.url)\n",
    "        file = request.files['file']\n",
    "        if file.filename == '':\n",
    "            return redirect(request.url)\n",
    "        upload_dir = 'static/uploads'\n",
    "        os.makedirs(upload_dir, exist_ok=True)\n",
    "        file_path = os.path.join(upload_dir, f\"upload_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg\")\n",
    "        file.save(file_path)\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            return \"Error: Could not load image.\", 400\n",
    "        emotion, emoji = predict_emotion(image)\n",
    "        print(f\"Saved image path: {file_path}\")\n",
    "        return render_template('result.html', emotion=emotion, emoji=emoji, image_path=file_path)\n",
    "    return render_template('upload.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
